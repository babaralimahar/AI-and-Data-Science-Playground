{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Real World Applications\n",
    "\n",
    "Before we dive into what Deep Learning is - let's review some applications of Deep Learning in our everyday life:\n",
    "\n",
    "1. __Self Driving Cars:__\n",
    "Vehicles like Teslas have networked cameras and various other sensors fitted around the car which provide vast amount of data that is fed to an AI model that learns to identify all sourounding objects and predict what the best next action for the vehicle is. The AI model simulates human perception and the descision making process using Deep Learning under the hood. \n",
    "\n",
    "<br>\n",
    "\n",
    "2. __Netflix Recommendation Engine:__\n",
    "A major contribution to Netflix's business model is its recommendation engine that enables customer rentention. Netflix gathers a vast amount of data of their user base including viewer ratings, viewing histories, wathing preferences etc - all of these data points are fed into machine learning models to generate models that can provide you a personalised recommendation based on your own preferences.  \n",
    "\n",
    "<br>\n",
    "\n",
    "3. __Face Recognition:__\n",
    "A multitude of applications are able to identify and tag people in pictures and videos. A task which is very easily performed by humans, even if faces change or are abstructed with accessories. However, until recently this has remained a challenging computer vision problem for many decades. These days, models are able to leverage Deep Learning and utilise big datasets of faces to learn similarities and develop face recognition capabiilties that even outperform humans. \n",
    "\n",
    "All of the above examples fall under the field of Artificial Intelligene which is essentially the theory and development of computer systems that perform tasks that would normally require human intelligence. Machine Learning is the application of AI based around the idea that computer systems should not be explicitely programmed but learn by experience and acquire skils without human intervention. \n",
    "\n",
    "<center><div> <img src=\"graphics/venn_diagram.png\" alt=\"Drawing\" style=\"width: 450px;\"/></div> </center>\n",
    "\n",
    "Deep Learning is a subfield of machine learning that uses __Artificial Neural Networks (ANN)__ at its backbone. ANNs are often described as mimicing the human brain through a set of algorthims. \n",
    "A major difference between machine learning models and ANNs is that the former lacks the mechanism to identify errors and requires human intervention to tune the model for more accurate decision. Whereas ANNs can identify the inaccurate decision and self-correct. \n",
    "\n",
    "At a very high level, a neural network comprises of __Input, Hidden and Output Layers__ that are made up of interconnected Nodes which resemble the way that biological neurons in the human brain signal to another.\n",
    "\n",
    "<center><div> <img src=\"graphics/deep_neural_network.png\" alt=\"Drawing\" style=\"width: 800px;\"/></div></center> \n",
    "Deep Learning and ANNs are often referred to interchangeably in conversation. The 'deep' in Deep Learning simply refers to the depth of the hidden layers and it is generally accepted that a neural network with more than three layers is considered a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 History of Deep Learning\n",
    "\n",
    "<center><div> <img src=\"graphics/dp_timeline.png\" alt=\"Drawing\" style=\"width: 1000px;\"/></div> </center>\n",
    "\n",
    "__1943:__ The first mathematical model of a neural network was created by Walter Pitts and Warren McCulloch in 1943 which demonstrated the thought process of the human brain. From here began the journey of deep neural network and deep learning.\n",
    "\n",
    "__1957:__ Frank Rosenblatt submitted a paper titled ‘The Perceptron: A Perceiving and Recognizing Automaton’, which consisted of an algorithm or a method for pattern recognition using a two-layer neural network. His work was mostly based on hardware rather than software. \n",
    "\n",
    "__1965:__ Alexey Ivakhnenko and V.G. Lapa developed the first working neural network. Afterwards, in 1971, Alexey Ivakhnenko created an 8-layer deep neural network which was demonstrated in the computer identification system, Alpha. This was the actual introduction to deep learning using statistical methods. \n",
    "\n",
    "__1979:__ Kunihiko Fukushima developed the ‘Neocognitron’, a multilayered ANN that will recognize visual patterns. His work served as the inspiraton of Convolution Neural Network a few decades later. \n",
    "\n",
    "__1985:__ Terry Sejnowski created NETtalk, a program which learnt how to pronounce English words. Around the same timeframe, David Rumelhart, Geoffrey Hinton, and Ronald J. Williams, released a paper entitle \"Learning Representations by Back-propagating Errors,” which described in greater detail the process of backpropagation. \n",
    "\n",
    "__1998:__ Yann LeCun published a paper entitled “Gradient-Based Learning Applied to Document Recognition”  which in conjunction with the backpropation alogrithm revolutionised an increasinly successfull approach to deep learning. A very popular dataset MNIST (a large database of handwritten digits) was used to show how deep learning models were able to learn abstract information by demonstrating that individual layers in the model identified different features, e.g. one layer finds circles and the next one would find edges etc.  \n",
    "\n",
    "__2009:__ As deep learning models require a tremendous amount of labelled data to train themselves in supervised learning, Fei-Fei Li launched ImageNet, which is a large database of labelled images (14 million) available to researches, educators and students. \n",
    " \n",
    "__2012:__ The results of ‘The Cat Experiment’ conducted by Google Brain were released. This experiment was based on unsupervised learning in which the deep neural network worked with unlabelled data to recognize patterns and features in the images of cats. However, it could only recognize 15% of images correctly.\n",
    "\n",
    "__2014:__ Facebook developed, DeepFace, a deep learning system to identify and tag faces of users in the photographs.\n",
    "\n",
    "The past decade has seen great leaps in the evolution of DL and its ability to learn more abstract information due to a combination of increased data, improved hardware and sofware. Today, Deep Learning is ever-present in our everyday lives (as shown in the previous examples) and as the field continues to evolve, humans become closer in their pursuit of AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Working of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have looked at where *Deep Learning (DL)* fits into the broader field of AI, and taken a brief look at its history.\n",
    "\n",
    "Before learning about the core of DL, the neural network, we will look at why and when we might want to use DL rather than other, simpler, *Machine Learning (ML)* algorithms.  \n",
    "\n",
    "First of all, DL has additional abilities compared to basic ML algorithms. It automatically learns representations from data without introducing any hand-coded rules or human domain knowledge. For example, to detect apples on a production line using machine learning, you might need to manually code the extraction of particular features like the roundness of an apple, its weight, or color, to help the machine to learn. DL needs no such instructions and can perform this *feature extraction* by itself.\n",
    "\n",
    "Said that, the choice between DL or ML can be based on the data we are analysing:\n",
    "\n",
    "* When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to extract and *understand* meaning patterns. In this case, a regular machine learning algorithm may deliver more accurate results.\n",
    "\n",
    "* When dealing with linear functions or less complex data, it can be easier and more efficient to consider alternative machine learning algorithms. If data is non-linear and complex, DL may be more suitable.\n",
    "\n",
    "We are now going to see how DL components work, starting with the fundamental building block of deep learning - the *perceptron*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network may have three types of layers:\n",
    "\n",
    "* **input layers** that take raw input from the domain,\n",
    "\n",
    "* **hidden layers** that take input from another layer and pass output to another layer,\n",
    "\n",
    "* and **output layers** that make a prediction.\n",
    "\n",
    "The *percepton* is a single-layer neural network with four main parameters, i.e., input values, weights and bias, net sum, and activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div> <img src=\"graphics/perceptron.png\" alt=\"Drawing\" style=\"width: 600px;\"/></div></center> \n",
    "\n",
    "The above diagram shows the forward propagation of information through a single neuron. Here the information is defined by its inputs, $x_{1}$, $x_{2}$..$x_{m}$ and their corresponding weights, $w_{m}$, $w_{2}$...$w_{m}$.\n",
    "We can then take the sum of the multiplication of the inputs and their respective weights (i.e., linear combination of inputs). A bias input is added to the summation, $Bias$ or $b$.\n",
    "\n",
    "This calculation results in a single number, which is fed to the activation function, where it generates the output. The bias value allows to shift the activation function regardless of the input, similar to the role of a constant $c$ in a linear function $y = mx+c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above forward propagation can be represented mathematically with the below function. \n",
    "<center><div> <img src=\"graphics/perceptron_equation.png\" alt=\"Drawing\" style=\"width: 300px;\"/></div> </center>\n",
    "\n",
    "Furthermore, we can use linear algebra to represent the linear combination of the inputs and its weight in vector format, and simply take the dot product of $X^{T}W$ to get the same result. \n",
    "<center><div> <img src=\"graphics/perceptron_equation_simplified.png\" alt=\"Drawing\" style=\"width: 300px;\"/></div> </center>\n",
    "\n",
    "We now dive into the activation function and its use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 2 Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *Activation Function* in a neural network decides the output value of a neuron. The choice of activation function has a large impact on the capability and performance of the neural network: the activation function in the *hidden layer* controls how well the network model learns the training dataset, while that on the *output layer* defines the type of predictions the model can make.\n",
    "\n",
    "The same activation function is usually used on all hidden layers, with a different activation function on the output layer; the choice on the latter depends on the type of prediction required by the model.\n",
    "\n",
    "Activation functions can be linear or non-linear. By using a non-linear activation function in the hidden layers of a neural network, the network can learn more complex patterns within the data.\n",
    "\n",
    "<center><div> <img src=\"graphics/non_linear_function.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> </center>\n",
    "\n",
    "In addition, neural networks are typically trained using the backpropagation of error algorithm that requires the derivative of a *'prediction error'* in order to update the weights of the model. Because of that, non-linear activation function is preferred. Let's have a look at an example.\n",
    "\n",
    "If we consider the linear function $y = x+c$, the calculated gradient for any neuron output would be the same, i.e., $1$.\n",
    "Therefore, we cannot apply backpropagation to find how the neuron weights should change based on the calculated error. Additionally, using linear activations would result in our model becomes a linear model because multiple linear functions chained together is still only considered to be a linear function. This would suggest that a linear activation function is not ideal, particularly when working with non-linear data.\n",
    "\n",
    "<center><div> <img src=\"graphics/linear_function_gradient.png\" alt=\"Drawing\" style=\"width: 400px;\"/></div> </center> \n",
    "\n",
    "Depending on the use case, we can use various non-linear activation functions. Among the most used:\n",
    "\n",
    "* Rectified Linear Activation Function\n",
    "\n",
    "* Sigmoid (or Logistic) Activation Function\n",
    "\n",
    "* Tanh Hidden Layer Activation Function\n",
    "\n",
    "Let's now have a quick look at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Rectified Linear Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rectified linear activation function is a piecewise (or hinge) linear function, meaning that it is linear for half of the input domain and non-linear for the other half. More specifically, it is linear for values greater than $0$ where it returns the value provided as input directly, and it is non-linear for values equal-to or less than $0$, for which it returns $0$. \n",
    "\n",
    "Below is the formula for the ReLu function:\n",
    "\n",
    "$$g(z) = max(z;0)$$\n",
    "\n",
    "and its graphical representation.\n",
    "\n",
    "<center><div> <img src=\"graphics/relu_function.png\" alt=\"Drawing\" style=\"width: 400px;\"/></div></center>\n",
    "\n",
    "A node or unit that implements this activation function is referred to as a rectified linear activation unit, or ReLU for short. Often, networks that use the rectifier function for the hidden layers are referred to as rectified networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Sigmoid (or Logistic) Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Sigmoid Activation Function* takes any real value as input and outputs values in the range $[0; 1]$. The larger the input (in the positive direction), the closer the output value is to $1$. And the smaller the input (in the negative direction), the closer the output to $0$.\n",
    "\n",
    "Below is the formula of the sigmoid activation:\n",
    "\n",
    "$$σ(x) = \\frac{1}{(1 + e^{-x})}$$\n",
    "\n",
    "and its graphical representation.\n",
    "\n",
    "<center><div> <img src=\"graphics/sigmoid_function.png\" alt=\"Drawing\" style=\"width: 400px;\"/></div></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Tanh Hidden Layer Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Tanh Hidden Layer Activation Function* takes any real value as input and outputs values in the range $[-1; 1]$. The larger the input (in the positive direction), the closer the output value to $1$. The smaller the input (in the negative direction), the closer the output is to $-1$.\n",
    "\n",
    "Below is the formula of the Tanh activation function:\n",
    "\n",
    "$$tanh(x) = \\frac{(e^{x} – e^{-x})}{(e^{x} + e^{-x})}$$\n",
    "\n",
    "and its graphical representation.\n",
    "\n",
    "<center><div> <img src=\"graphics/tanh_function.png\" alt=\"Drawing\" style=\"width: 400px;\"/></div></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Shallow Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our understanding of the *perceptron* and create a *Shallow Neural Network*, characterised by a single hidden layer with multiple neurons (bias has been ignored for simplicity), that feed into an output layer. This differs from a *Deep Neural Network*, which characterised by several hidden layers (often of various types).\n",
    "\n",
    "<center><div> <img src=\"graphics/shallow_nn_1.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div></center> \n",
    "\n",
    "The layer is *'hidden'* as its state is not directly observable, whereas the input and the output layers are directly observable. We can probe to find the state, but they are learned rather than enforced (we will discuss this later).\n",
    "\n",
    "There are two transformation processes with their respective weight matrices denoted as $W^{\\left( 1\\right) }$ (between the input and hidden layers) and $W^{\\left( 2\\right) }$ (between the hidden and output layers).\n",
    "\n",
    "We can then perform a single perceptron computation in the hidden layer with\n",
    "$z_{i}=w_{0,j}^{\\left( 1\\right) }+\\sum ^{m}_{j=1}x_{j}w_{j,i}^{\\left( 1\\right) }$. This computation is the weighted sum of all of its inputs, transformed by a non-linearity function($g\\left( z_{i}\\right)$). The result is then passed on to an output layer node which is calculated in a similar manner: $\\widehat{y}_{i}=g\\left( w_{0,i}^{\\left( 2\\right) }+\\sum ^{d}_{j=1}z_{j}w_{j,i}^{\\left( 2\\right) }\\ \\right)$.\n",
    "\n",
    "For demonstration, $z_2$:\n",
    "\n",
    "<center><div> <img src=\"graphics/shallow_nn_example.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> </center>\n",
    "\n",
    "$$z_{2}=w_{0,2}^{\\left( 1\\right) }+\\sum ^{m}_{j=1}x_{j}w_{j,2}^{\\left( 1\\right) }$$\n",
    "\n",
    "$$z_{2}=w_{0,2}^{\\left(1\\right)}+ x_1w_{1,2}^{\\left( 1\\right)} + x_2w_{2,2}^{\\left( 1\\right)}  + x_mw_{m,2}^{\\left( 1\\right)}  $$\n",
    "\n",
    "The same process is repeated for every neuron in the hidden layer and their results are forward propagated as an input to the nodes in the next layer (in this case the output layer). We will going to demonstrate this with an example in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Shallow Neural Network Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div> <img src=\"graphics/shallow_nn_applied.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> </center>\n",
    "\n",
    "In this example we are trying to predict the propability of someone understanding deep learning based on two features:\n",
    "\n",
    "1. The number of courses a person has completed.\n",
    "\n",
    "2. The average number of hours spent on each course.\n",
    "\n",
    "Our example input is *2* courses and an average of *3* hours spent on each course. \n",
    "\n",
    "<center><div> <img src=\"graphics/shallow_nn_applied_2.png\" alt=\"Drawing\" style=\"width: 750px;\"/></div> </center>\n",
    "\n",
    "You can see that our network is not very good at predicting the correct probability. That is due to the fact that our network weights are completely randomised as the network has not yet been trained. To improve the prediction, we need to find weights so that the $Error$ is reduced to nearly *0*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the model to correct itself, it will need to find a way to measure its error. This is defined as the loss of the network, which measures the difference between predictions and actual values. Note that $f(x)$ below denotes the final model output for input values, $x$.\n",
    "\n",
    "<center><div> <img src=\"graphics/loss_function.png\" alt=\"Drawing\" style=\"width: 250px;\"/></div> </center>\n",
    "\n",
    "The input to the loss function are the $Prediction$ and the $Actual$ values. The closer the difference between those values, the better the model - it is therefore  necessary to try and minimise the loss $L$.\n",
    "\n",
    "The empirical loss $J(W)$ (also known as the cost function) measures the total average loss across all of our data points, not just one sample. It is computed by finding the mean of the loss of all our data points and their respective predictions.\n",
    "\n",
    "<center><div> <img src=\"graphics/cost_function.png\" alt=\"Drawing\" style=\"width: 650px;\"/></div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to train the network using the loss function to find the weights $W^*$ that achieve the lowest loss. This can be mathematically represented as such:\n",
    "\n",
    "$$W^{\\ast }=\\argmin_{W}\\dfrac{1}{n}\\sum ^{n}_{i=1}L\\left( f\\left( x^{\\left( i\\right) };W\\right) ,y^{\\left( i\\right) }\\right)$$\n",
    "$$W^{\\ast }=\\argmin_{W}J\\left( W\\right)$$\n",
    "$$W=\\left\\{ W^{\\left( 0\\right) },W^{\\left( 1\\right) },\\ldots \\right\\}$$\n",
    "\n",
    "$W$ is the collection of all the weights across all layers. \n",
    "\n",
    "The loss can be described as a function of our weights, and for the purposes of visualisation, the loss landscape created across two weights $w_{0}$ and $w_{1}$ would look like this:\n",
    "\n",
    "<center><div> <img src=\"graphics/parameter_landscape.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> </center>\n",
    "\n",
    "This landscape shows us the loss that is expected for all varitions of $w_{0}$ and $w_{1}$. The whole process of training the model is to establish the optimal $w_{0}$ and $w_{1}$, i.e., $ W^{\\ast }$ that would provide the lowest loss. \n",
    "\n",
    "The best way to do that is by randomly picking a point $(w_{0},w_{1})$ in the landscape. At that point, we need to determine the slope of the angle of our $J(W)$ to understand if we are headed in the right direction. A negative slope means that we are headed downward, while a posive slope means that we moved beyond the minimum. To determine the slope, we compute the gradient of the function on that point, $\\dfrac{\\partial J\\left( W\\right) }{\\partial W}$. Given the gradient can be interpreted as the direction and rate of fastest increase, we will need to take a small step in the opposite direction to find a lower loss. We repeat this process until convergence, i.e., we hit a local minimum. This process is anologous of letting a ball loose at a random point on a hill and it coming to a stop at the bottom (global minimum) or in a small indent higher up the hill (local minima).\n",
    "\n",
    "<center><div> <img src=\"graphics/global_minimum.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> </center>\n",
    "\n",
    "Note that it is ulikely to find the global minimum by selecting a single random point. To increase the chance of finding it, we could choose different points of the loss function and follow the same steps for all of them. Such an approach is called *random restart*.\n",
    "\n",
    "The above algorithm is called __Gradient Descent__ and can be summarised as follows:\n",
    "\n",
    "1. Initialize weights randomly.\n",
    "\n",
    "2. Loop until convergence:\n",
    "\n",
    "    2.a. Compute gradient, $\\dfrac{\\partial J\\left( W\\right) }{\\partial W}$.\n",
    "\n",
    "    2.b. Update weights, $W\\leftarrow W-\\alpha\\dfrac{\\partial J\\left( W\\right) }{\\partial W}$.\n",
    "    \n",
    "    2.c. Return weights.\n",
    "\n",
    "Note that $\\alpha$ is the *learning rate*, a factor used to control the speed at which the model learns.\n",
    "\n",
    "The choice of the learning rate is relevant:\n",
    "\n",
    "* Lower learning rates result in smaller changes. Smaller changes may allow the model to learn a more optimal set of weights, but may take longer to train.\n",
    "\n",
    "* Too low learning rates may lead the model to never converge or get stuck on a suboptimal set of weights.\n",
    "\n",
    "* Higher learning rates result in bigger changes. Bigger changes may allow the model to learn faster, at the cost of never converging or arriving on a suboptimal final set of weights\n",
    "\n",
    "* Too large learning rates may result in weights that will be too large and the model will generate divergent oscillations. In this case, gradient descent can inadvertently increase rather than decrease the training error.\n",
    "\n",
    "Therefore, we should be careful to choose a learning rate that is not too small or large. We need to choose the one that allows us to find, on average, a good-enough set of weights.\n",
    "\n",
    "We have now seen how a simple *Shallow Neural Network* is assembled and trained to optimise a model that will yield a prediction with the lowest error. \n",
    "\n",
    "In the next section, we are going to use Python to create a model to predict house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Code Walkthrough\n",
    "\n",
    "In this notebook we will work through a code example of a simple and shallow neural net. Before doing so, we should consider the *business use-case*. Our scenario will look at house price prediction, is there a business use case here?\n",
    "\n",
    "Of course, the ability to use AI-powered prediction is a real advantage. If done well, an estate agent that uses accurate predictions for the properties they deal with has a big advantage over an estate agent that uses human experts to make these predicitons, the AI-powered company can operate much more efficiently, consider:\n",
    "\n",
    "A new property is brought to the estate agent, a form describing the property is filled out and entered into a database.\n",
    "\n",
    "* The traditional company must then pay a human expert to make an initial price estimate, this can take time and estimates can be affected by human error or even the mood of that person on that day. If the expert is unfamiliar with the area the house is located in, or typically deals with smaller houses, their prediction take longer (costing the estate agent more money), and be inaccurate.\n",
    "\n",
    "* The AI-powered company has a program that automatically feeds this new data into the house prediction neural net, returning an estimate almost instantaneously. The neural net can do this for millions of new houses almost every second, the costs of running this neural net are minimal.\n",
    "\n",
    "Already we can see the potential of something like this, however, it must be done carefully and a human may still need provide a final estimate. There is the case of *Zillow*, a real-estate marketplace that used AI to power a rapid home buying and reselling tool called *\"Zillow Offers\"*. Zillow Offers was their AI-powered house price estimation tool, that provided start-to-end house pricing with little human oversight, unfortunately this turned out very poorly for Zillow who quickly shut down Zillow Offers, wrote off $569M worth of homes, and laid off 25% of its staff [\\[source\\]](https://www.cnet.com/personal-finance/mortgages/what-happened-at-zillow-how-a-prized-real-estate-site-lost-at-ibuying/).\n",
    "\n",
    "On the other hand, other companies such as Opendoor and Offerpad seem to have implemented AI powered price estimates with much more success [\\[source\\]](https://www.protocol.com/manuals/proptechs-big-moment/ibuyer-opendoor-offerpad-zillow), showing that this approach can be implemented successfully, if done right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 2.4.0\n",
      "ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\n",
      "ERROR: Could not find a version that satisfies the requirement numpy==1.21.5 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.3.5, 2.4.0rc1, 2.4.1, 2.4.2)\n",
      "ERROR: No matching distribution found for numpy==1.21.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy==1.21.5\n",
    "# !pip install pandas==1.3.5\n",
    "# !pip install scikit_learn==1.0.2\n",
    "# !pip install openpyxl==3.0.9\n",
    "# !pip install matplotlib==3.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# to show all the generated plots inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of house price data for properties in Pune, India. We have 200 individual data points across 17 different features. Our aim is to see if we can build a simple *Deep Learning Model* using *Numpy* only. Note that we won't be using *PyTorch* at this stage - this will be covered in a future course. \n",
    "\n",
    "We will start by loading and visualising the data, analysing the variable types, and selecting the variables needed.\n",
    "\n",
    "See the *DataDescription.pdf* for each variable's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_excel('../Input/Real_Estate_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the top rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types in our dataframe:\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the dataset, we can see that we have a mixture of data types, some are quantitative (i.e., *'int64'*, *'float64'*), but the majority qualitative in nature (*'object'*). To run our *Neural Network Model (Multilayer Perceptrons)*, we will need to have floating variables only (*'float64'*) as these are the only variable type the neural network can perform computation on; it can also work with integer numbers, but it will convert them into floating. \n",
    "\n",
    "Therefore, in the next section, we are going to clean our main dataset to improve our data quality and be able to run the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section, we are not going to clean our dataset. \n",
    "Specifically, we are going to follow the below steps:\n",
    "1. Extract the relevant variables.\n",
    "2. Remove rows containing *'NaN'* values, if any.\n",
    "3. Convert qualitative variables into quantitative, i.e., from *'object'* to *'float64'*.\n",
    "4. Scale variables, if needed.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Extract the Relevant Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For our model, we are going to select 5 variables that are probably the most relevant:\n",
    "1. Propert Type\n",
    "2. Property Area in Sq. Ft.\n",
    "3. Sub-Area\n",
    "4. Swimming Pool\n",
    "5. Price in Millions\n",
    "\n",
    "Important to note, *'Price in Millions'* is the variable we want to predict, therefore, we will split it from the dataset afterwards.\n",
    "\n",
    "Let's start by selecting those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables selection\n",
    "df_selection = df[['Propert Type','Property Area in Sq. Ft.','Sub-Area','Swimming Pool','Price in Millions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Remove Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data can be handle in different ways, e.g., replacing, interpolating, or removing data, depending on the size of the dataset, and the usefulness of that data point. In this case, we are going to remove each row containing *missing data*, or *NaN*, to avoid errors when running the model.\n",
    "\n",
    "Let's check if we have any missing data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if we have any 'NaN' in the dataset\n",
    "nan_rows = df_selection[df_selection.isnull().any(axis = 1)]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that row 41 includes missing values, therefore, we are going to drop them using the *`drop`* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given we do have 'NaN' values, let's store them in a new variable ('nan_rows') and drop their respective rows\n",
    "nan_rows = nan_rows.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the respective rows\n",
    "df_selection = df_selection.drop(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now check that all rows containing NaN values have been removed\n",
    "df_selection[df_selection.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Variables Conversion from Qualitative to Quantitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables extracted from the original database, except for `'Price in Millions'`, are qualitative variables. \n",
    "Therefore, we will need to convert them into quantitative variables.\n",
    "\n",
    "Let's start with the first variable, `'Propert Type'`, which, as the header suggests, represents the property type (i.e., 1 BHK is 1 bedroom, hall, and kitchen, 2 BHK is 2 bedroom, hall, and kitchen and so on). To convert the variable into *float*, we are going to extract the number included in the variable's value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the numerical value for the 'Propert Type'\n",
    "df_selection['Propert Type'] = df_selection['Propert Type'].str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must be aware that, this operation could have generated additional missing values. In the case one variable's value didn't include a number, but text only, the resulting value would be *NaN*. Therefore, we need to check once again, if missing values have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if we have any 'NaN' in the dataset\n",
    "nan_rows = df_selection[df_selection.isnull().any(axis = 1)]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! As expected, row 51 is now *NaN*. Let's drop it and go on with the data cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given we do have 'NaN' values, let's store them in a new variable ('nan_rows') and drop their respective rows\n",
    "nan_rows = nan_rows.index.values\n",
    "# drop the respective rows\n",
    "df_selection = df_selection.drop(nan_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move to the next variable, `'Sub-Area`.\n",
    "In this case, we want to convert the variable, which is a categorical variable, into a format that can be readily used by our neural network. To do so, we are using *'one-hot encoding'*, which helps us creating new binary variables (i.e.,including only True or False) to represent the original categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting our categorical variables using 'one-hot encoding' \n",
    "area_encoded = pd.get_dummies(df_selection['Sub-Area'].str.lower().str.strip(), prefix = 'area', dtype = 'float64')\n",
    "\n",
    "# check the categorical variables\n",
    "area_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then concatenate the data base to our new binary variables using *`concat`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate df_selection with encoded variables\n",
    "df_selection = pd.concat([df_selection,area_encoded], axis = 1)\n",
    "\n",
    "# check the updated dataset\n",
    "df_selection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy with the result, we can now remove the `'Sub-Area'` variable, using again the *`drop`* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'Sub-Area' variable\n",
    "df_selection = df_selection.drop('Sub-Area', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally have a look at the last variable, `'Swimming Pool'`. In particular, we want to analyse the its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection['Swimming Pool'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the array contains different values (some in lowercase, some in uppercase, and others with unnecessary space). Therefore, we need to (1) convert the values in lowercase using *`lower()`*, (2) remove any space using *`strip()`*, and (3) convert from string to binary using *`map()`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 'Swimming Pool' variable from string to binary\n",
    "df_selection['Swimming Pool'] = df_selection['Swimming Pool'].str.lower().str.strip().map(\n",
    "    {'yes': True , 'no': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have converted correctly the variable `'Swimming Pool'` to binary, so that its values are True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the conversion happened correctly\n",
    "df_selection['Swimming Pool'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we achieved what we wanted! We can then move to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Variables Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step consists of scaling (i.e., normalising between 0 and 1) the `'Property Area in Sq.Ft.'` as its values are too large compared to the other variables' values; this would overpower the other inputs' activations resulting in an unfair representation in the network. To do so, we are going to use the *`MinMaxScaler()`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the 'Property Area in Sq.Ft.' variable \n",
    "scaler = MinMaxScaler()\n",
    "df_selection['Property Area in Sq. Ft.'] = scaler.fit_transform(df_selection[['Property Area in Sq. Ft.']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the resulting scaled variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection['Property Area in Sq. Ft.'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Clean Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should have cleaned our dataset, which should contain only float variables.\n",
    "Let's ensure that this is true by converting all values to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring our variables are float\n",
    "df_selection['Propert Type'] = df_selection['Propert Type'].astype(float, errors = 'raise')\n",
    "df_selection['Property Area in Sq. Ft.'] = df_selection['Property Area in Sq. Ft.'].astype(float, errors = 'raise')\n",
    "df_selection['Swimming Pool'] = df_selection['Swimming Pool'].astype(float, errors = 'raise')  \n",
    "df_selection['Price in Millions'] = df_selection['Price in Millions'].astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_selection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy with the result, we can split our dataset in 2: one including all variables except for the `'Price in Millions'`, and one including only `'Price in Millions'`. This step will allow us to compare the variable predicted with the model to the actual variable, which is `'Price in Millions'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset including all variables, but 'Price in Millions'\n",
    "X = df_selection.drop(['Price in Millions'], axis = 1)\n",
    "print(X.shape)\n",
    "\n",
    "# dataset including only 'Price in Millions'\n",
    "Y = df_selection['Price in Millions']\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, we will need to perform array operations; therefore, if we have arrays with different shapes, we need to reshape them to make them comparable and be able to perform element-wise operations. This is the case of the variable `'Y'`, which has currently the shape (198,), but should be (198,1). Let's reshape it using *`reshape`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the array\n",
    "Y = Y.values.reshape((Y.shape[0], 1)) \n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create *test data* and *training data*. Test data excludes the first 150 rows, while test data includes the last 48 rows. Also in this case, we need to transpose the data to be able to perfrom array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data and shape\n",
    "X_test=X[150:].T  \n",
    "Y_test=Y[150:].T  \n",
    "print('X_test:', X_test.shape)\n",
    "print('Y_test:', Y_test.shape)\n",
    "\n",
    "#training data and shape\n",
    "X_train = X[:150].T\n",
    "Y_train = Y[:150].T\n",
    "print('X_train:', X_train.shape)\n",
    "print('Y_train:', Y_train.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move to the next section, where we will start looking at the model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Single-Layer Neural Network Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going through the *Single-Layer* computation, the simplest form of neural network. Defined like that as there is only one layer of input nodes that send weighted inputs to a subsequent layer of receiving nodes.\n",
    "\n",
    "We will start with the forward-propagation and finish with backward propagation. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.4.1 Forward-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is based on the below two steps:\n",
    "1. *Inputs initialization*. Weighted sum of inputs from the previous layer plus the bias.\n",
    "\n",
    "2. *Activation*. The calculated weighted sum of inputs is passed to the activation function to add non-linearity to the network. Among the most popular activation functions, we are considering the *Rectified Linear Unit (ReLU) activation function*.\n",
    "\n",
    "With the *inputs initialization*, we want to initialize the weights (`W`), the bias (`b`), and the resultant output (`Z`) from these.\n",
    " \n",
    "The weight initialization usually depends on the activation function considered in the analysis. The standard approach when using a ReLU activation function is the *he* initialization method.\n",
    "\n",
    "This method generates weights as a random number with a Gaussian probability distribution (G) with a mean of 0.0 and a standard deviation of sqrt(2/n), where n is the number of inputs to the node.\n",
    "\n",
    "weight ~ G (0.0, sqrt(2/n))\n",
    "\n",
    "In our example, n equals 36.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 'he' initialization\n",
    "n_in = X_train\n",
    "n_out = Y_train\n",
    "n = n_in.shape[0] # this equals to 36\n",
    "print(n)\n",
    "# calculate the standard deviation\n",
    "std = np.sqrt(2.0/n_out.shape[0])\n",
    "# calculate the weights\n",
    "W = np.random.randn(n_out.shape[0], n_in.shape[0]) * std\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we initialized the weights, we should do the same with the bias (`b`). In this case, we will set it to zero, for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize bias 'b' as zero\n",
    "b = np.zeros((n_out.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to perform a *forward-propagation*, which refers to the calculation and storage of intermediate variable (`Z`) for a neural network from the input layer to the output layer.\n",
    "\n",
    "$$Z = X^{T}W + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the intermediate variable\n",
    "Z = np.dot(W, n_in) + b\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pass to the *activation*. The scope is to process the output $Z$ through an *activation function*. As mentioned, we have chosen to use the *ReLU* function for our activation. This is a linear function that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "\n",
    "The output $\\hat{Y}$ represents our *prediction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU function\n",
    "Y_hat = np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to compare the prediction $\\hat{Y}$ to a *true* value $Y$ to calculate the *Mean Squared Error (MSE)* between predicted and true values. This can be calculated using the Loss Function $L(\\hat{Y},Y)$.\n",
    "$$MSE = L(\\hat{Y},Y) = \\frac{1}{n}\\ (\\hat{Y} - Y)^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the true value 'Y' and the sample size 'n'\n",
    "Y = Y_train\n",
    "n = Y_hat.shape[1]\n",
    "\n",
    "# calculate the mean squared error or loss function\n",
    "E = (1 / n) * np.sum(np.square(Y_hat - Y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the error before training our neural network\n",
    "E, E.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Backward-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not stepping into the backward-propagation. This is a way of propagating the total loss back into the neural network to see how much of the loss every node is responsible for, and then updating the weights to minimize the loss by giving the nodes with higher error, lower weights and vice-versa.\n",
    "\n",
    "First, we need to derive the slope of the function to understand the direction we need to go to. If the slope is positive, then we would need to move to the right; otherwise, we would need to move to the left. \n",
    "\n",
    "To do that, we can calculate the derivative of the mean squared error (MSE) with respect to the output activation $\\hat{Y}$.\n",
    "\n",
    "\n",
    "$$\\dfrac{\\partial E }{\\partial \\hat{Y}}=\\dfrac{2}{n}\\left( \\hat{Y}- Y\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of the loss function\n",
    "dE = (2 / n) * (Y_hat - Y)\n",
    "dE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we need calculate the derivative of the ReLU function to update the weights of a node as part of the backpropagation of error.\n",
    "\n",
    "The derivative of the ReLU function is the slope. The slope for negative values is 0.0 and the slope for positive values is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of the ReLU function\n",
    "dY_hat = (Z > 0)\n",
    "dY_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to optimize the weights? We can use a backward propagation with gradient descent.\n",
    "\n",
    "The idea is to calculate gradients of the loss function with respect to the model weights and propagate them back layer by layer. This way, the model knows the weights, responsible for creating a larger error, and tunes them accordingly.\n",
    "\n",
    "What we are aiming is minimize the cost. Gradient indicates the direction of increase. As we want to find the minimum point, we need to go in the opposite direction of the gradient. We update parameters in the negative gradient direction to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data \n",
    "dZ = n_in\n",
    "dZ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradients of the loss function with respect to the weights\n",
    "dW = np.dot((dE * dY_hat),dZ.T)\n",
    "dW, dW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we increase/decrease the size of weight updates? We can use the *learning rate* $\\alpha$.\n",
    "\n",
    "Unfortunately, we cannot calculate the optimal learning rate, but we can derive a good-enough learning rate via trial and error to find, on average, a good-enough set of weights. Typically, the range of values to consider for the learning rate is less than $1$ and greater than $10^{-6}$. \n",
    "\n",
    "After running the model several times, we chose $\\alpha = 0.1$, and used it for the below weight optimization equation.\n",
    "\n",
    "$$W = W - \\alpha \\dfrac{\\partial L }{\\partial w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 'learning rate'\n",
    "alpha = 0.1\n",
    "\n",
    "# update the weights using the learning rate\n",
    "W = W - (alpha * dW)\n",
    "\n",
    "# see updated weights\n",
    "W, W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for the weights, we need now to calculate the derivative of the loss function, but with respect to the bias ($b$). After, we need to update the bias using the learning rate ($\\alpha$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of loss function with respect to bias\n",
    "db = np.sum((dE * dY_hat), axis = 1, keepdims = True)\n",
    "\n",
    "# update bias using the learning rate\n",
    "b = b - (db * alpha) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculations represent an *epoch*, i.e., an entire loop through the forward and back-propagation process using every sample in the training set; however, only one epoch will lead to *underfitting error*. That is why, to get to an optimal model, we need to increase the number of epochs. Note that using too many epochs might result in *overfitting error* instead. There is no correct number of epochs, it depends on the size and type of dataset and model. In this example, we are going to use 100 epochs, as it seems to give us the optimal curve.\n",
    "\n",
    "Let's now iterate the process to *optimize* our weights and predict a value $\\hat{Y}$ closer to the true value $Y$. To do so, we are going to use the *`for`* loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to store the error calculated in each iteration\n",
    "error = []\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 100 \n",
    "\n",
    "# iteration\n",
    "for i in range(epochs):\n",
    "\n",
    "    # initialize the intermediate variable\n",
    "    Z = np.dot(W, n_in) + b\n",
    "    \n",
    "    # forward-propagation\n",
    "    # ReLU activation\n",
    "    Y_hat = np.maximum(0,Z) \n",
    "    \n",
    "    # define the sample size 'n'\n",
    "    # note that the true value 'Y' is already defined in the above calculation - no need to define it again.\n",
    "    n = Y_hat.shape[1] \n",
    "     \n",
    "    # calculate the mean squared error or loss function\n",
    "    E = (1 / n) * np.sum(np.square(Y_hat - Y))\n",
    "\n",
    "    # backward-propagation\n",
    "    # derivative of the loss function\n",
    "    dE = (2 / n) * (Y_hat - Y) \n",
    "    \n",
    "    # derivative of Relu Activation function\n",
    "    dY_hat = (Z > 0)\n",
    "\n",
    "    # input data \n",
    "    dZ = n_in  \n",
    "\n",
    "    # derivative of loss function with respect to weights\n",
    "    dW = np.dot((dE * dY_hat),dZ.T)\n",
    "    \n",
    "    # derivative of loss function with respect to bias\n",
    "    db = np.sum((dE * dY_hat), axis = 1, keepdims = True)\n",
    "    \n",
    "    # update weights\n",
    "    W = W - (dW * alpha)\n",
    "    \n",
    "    # update bias\n",
    "    b = b - (db * alpha) \n",
    "\n",
    "    # error\n",
    "    error.append(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the error using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"Learning Rate = \" + str(alpha))\n",
    "plt.plot(np.squeeze(error))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to apply the knowledge from the trained neural network model on the test data and use it to infer the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now push our test set through the neural network with our learned weights and bias \n",
    "Z_predict = np.dot(W, X_test) + b  \n",
    "\n",
    "# activation function\n",
    "Y_hat_predict=np.maximum(0,Z_predict)\n",
    "\n",
    "# define the size\n",
    "n = X_test.shape[1]  \n",
    "\n",
    "# error prediction\n",
    "E_predict = (1 / n) * np.sum(np.square(Y_hat_predict - Y_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the error after training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated error\n",
    "E_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted values vs. the actual values\n",
    "plt.ylabel('Price in Millions')\n",
    "plt.xlabel('Sample')\n",
    "plt.title(\"Predicted vs. Actual\")\n",
    "plt.plot(np.squeeze(Y_hat_predict),label = \"Predicted\")\n",
    "plt.plot(np.squeeze(Y_test), label = \"Actual\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from even this very simple example that a neural net can be setup to train and update its own weights to improve upon its error/cost. \n",
    "\n",
    "However, you can also see that using the current single hidden layer network, our predictions are generally correlated but not too accurate. We will cover this in more depth in future chapters, but the reason for this is that our network is unable to represent complex relationships with only a few weights. However, when we take the same concepts discussed above and apply them across multiple layers with more neurons and adjustable weights/biases more complex relationships can be represented. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babar_ali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
